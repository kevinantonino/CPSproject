{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "PV1_all = pd.read_csv('Data\\enel_data\\PV_data\\PV1.csv')  #read the PV1 data file to get the solar generation data\n",
    "PV1_filtered = PV1_all[PV1_all['Alarm'] == 0] # Filter out zeros \n",
    "PV1_filtered[\"V\"]=PV1_filtered[\"V\"].apply(pd.to_numeric) #ensure all voltage laues are numeric\n",
    "\n",
    "\n",
    "\n",
    "PV1_filtered = PV1_filtered[PV1_filtered[\"V\"] < 214748360]  #filter out misreadins as enel tells us to in their email\n",
    "PV1_filtered = PV1_filtered[['timestamp','I_shunt','V']]\n",
    "PV1_filtered[\"Power_kW\"]= PV1_filtered['I_shunt']*PV1_filtered['V']/1000 #Power = I*V   (dived by 1000 to get units of kw)\n",
    "PV1_filtered['timestamp'] = pd.to_datetime(PV1_filtered['timestamp'],errors = 'coerce') # change timestamps from strings to datetime objects so we can index and resample. takes like 5 min \n",
    "PV1_filtered.index=PV1_filtered[\"timestamp\"] # set index to be our datetime so we can resample\n",
    "\n",
    "PV1_resampled=PV1_filtered.resample('15T').mean() #resample power reading to the mean of every 15 minute interval\n",
    "\n",
    "#need to isolate the timestamp from the year it was taken so we can merge with the timestamps of a different year for the house load data (since we do not have solar data available for every house)\n",
    "PV1_resampled[\"month1\"]=pd.DatetimeIndex(PV1_resampled.index).month.astype(str) #create month column\n",
    "PV1_resampled[\"day1\"]=pd.DatetimeIndex(PV1_resampled.index).day.astype(str) #create day column\n",
    "PV1_resampled[\"time1\"]=pd.DatetimeIndex(PV1_resampled.index).time.astype(str) # create time column\n",
    "\n",
    "\n",
    "\n",
    "PV1_resampled['marker1'] = PV1_resampled[['month1', 'day1', \"time1\"]].agg('-'.join, axis=1) #create our mark column that we will use to merge this df with the house load df later\n",
    "PV1_resampled=PV1_resampled.drop(columns=[\"I_shunt\",\"V\"]) #we no longer need these values\n",
    "\n",
    "# PV2_all = pd.read_csv('Data\\enel_data\\PV_data\\PV2.csv')\n",
    "# PV2_filtered = PV2_all[PV2_all['Alarm'] == 0] # Filter out zeros \n",
    "# PV2_filtered[\"V\"]=PV2_filtered[\"V\"].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# PV2_filtered = PV2_filtered[PV2_filtered[\"V\"] < 214748360]\n",
    "# PV2_filtered = PV2_filtered[['timestamp','I_shunt','V']]\n",
    "# PV2_filtered[\"Power_kW\"]= PV2_filtered['I_shunt']*PV2_filtered['V']/1000\n",
    "\n",
    "# PV2_filtered['timestamp'] = pd.to_datetime(PV2_filtered['timestamp'],errors = 'coerce') # takes like 5 min \n",
    "# PV2_filtered.index=PV2_filtered[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of files we want to sort through\n",
    "\n",
    "files=[\"file_201907.csv\",\"file_201906.csv\",\"file_201905.csv\",\"file_201904.csv\",\"file_201903.csv\",\"file_201902.csv\",\"file_201901.csv\",\"file_201812.csv\",\"file_201811.csv\",\"file_201810.csv\",\"file_201809.csv\",\"file_201808.csv\"]\n",
    "test=[\"file_201907.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe that will be appended with more house data from the following loops\n",
    "\n",
    "cleaned_data=pd.DataFrame(columns=[\"dataid\",\"local_15min\",\"load\",\"solar\"])\n",
    "\n",
    "# df.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012]\n",
      "[118000000008 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n"
     ]
    }
   ],
   "source": [
    "for file in files: #iterate through all files given above\n",
    "    \n",
    "    filepath= \"Data\\\\enel_data\\\\NILM_data\\\\\" + file  \n",
    "                      \n",
    "    all_data = pd.read_csv(filepath,delimiter = ';',encoding = 'utf8') #read this file\n",
    "    \n",
    "    serial_buffer=all_data[all_data['Serial_Number'] != \"Serial_Number\"][['Serial_Number','Data','Attiva']] #get rid of erroneous rows\n",
    "    serial_buffer=serial_buffer[serial_buffer[\"Attiva\"].map(type)== int]\n",
    "    \n",
    "    serial_buffer['Serial_Number']=pd.to_numeric(serial_buffer['Serial_Number']) #someserial numbers are given as strings so lets convert all to inttegers\n",
    "    serial_buffer['Data'] = pd.to_datetime(serial_buffer['Data'],errors = 'coerce') # convert timestamp to datetime object.  takes like 5 min \n",
    "    house_ids=serial_buffer[\"Serial_Number\"].unique() #get a list of all Serial Numbers (houses) in this file\n",
    "\n",
    "    print(house_ids)\n",
    "    \n",
    "    for house_id in house_ids: #do a new interation for each separate home that we found in this file\n",
    "        \n",
    "        house= serial_buffer[serial_buffer['Serial_Number'] == house_id] #select data for this home\n",
    "        house = house.rename(columns={'Data':'time','Attiva':'load'}) #load is in mW\n",
    "        house.index = house['time'] #change index \n",
    "        house = house.drop(columns='time')\n",
    "        house[\"load\"] = house[\"load\"].apply(pd.to_numeric, errors='coerce') #make all load values numeric\n",
    "        house = house.resample('15T').mean() #resample house load to average load every 15 min\n",
    "\n",
    "        house['load'] = house['load'] / 1000 / 1000 # convert mW to kW\n",
    "        \n",
    "        \n",
    "        house[\"t_stamp\"]=house.index #make new column of datetime we can break apart\n",
    "        \n",
    "        #same as PV data: make a marker we will use to merge the two df by taking the year out of the timestamp\n",
    "        house[\"month2\"]=pd.DatetimeIndex(house.index).month.astype(str)\n",
    "        house[\"day2\"]=pd.DatetimeIndex(house.index).day.astype(str)\n",
    "        house[\"timey2\"]=pd.DatetimeIndex(house.index).time.astype(str)\n",
    "\n",
    "        house['marker2'] = house[['month2', 'day2', \"timey2\"]].agg('-'.join, axis=1) #create the second marker\n",
    "        \n",
    "        merged= pd.merge(house,PV1_resampled, how='inner', left_on=\"marker2\", right_on=\"marker1\") #merge the two df using the markers we made. data from the same realtive date and time interval will be combined\n",
    "        df_append= merged.loc[:,[\"Serial_Number\",\"t_stamp\",\"load\",\"Power_kW\"]] #select the columns we want to keep\n",
    "        df_append = df_append.rename(columns={'Serial_Number':'dataid','t_stamp':'local_15min','Power_kW':'solar'})\n",
    "        \n",
    "        cleaned_data=cleaned_data.append(df_append, ignore_index=True) #add the merged load and solar data for all homes in this file to the larger df we are creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans=cleaned_data[pd.notna(cleaned_data[\"dataid\"])]\n",
    "no_nans = no_nans.rename(columns={'local_15min':'time'})\n",
    "no_nans[\"car1\"] = 0\n",
    "no_nans[\"dataid\"]= no_nans[\"dataid\"].apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataid</th>\n",
       "      <th>time</th>\n",
       "      <th>load</th>\n",
       "      <th>solar</th>\n",
       "      <th>car1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:00:00</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:15:00</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:30:00</td>\n",
       "      <td>0.079875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:45:00</td>\n",
       "      <td>0.066261</td>\n",
       "      <td>-0.001313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 01:00:00</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663707</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 16:30:00</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663708</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 16:45:00</td>\n",
       "      <td>0.131403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663709</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:00:00</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663710</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:15:00</td>\n",
       "      <td>0.056625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663711</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:30:00</td>\n",
       "      <td>0.054383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38370 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataid                time      load     solar  car1\n",
       "0       118000000008 2019-01-07 00:00:00  0.077571 -0.002998     0\n",
       "1       118000000008 2019-01-07 00:15:00  0.070403 -0.000574     0\n",
       "2       118000000008 2019-01-07 00:30:00  0.079875  0.000000     0\n",
       "3       118000000008 2019-01-07 00:45:00  0.066261 -0.001313     0\n",
       "4       118000000008 2019-01-07 01:00:00  0.070168 -0.002968     0\n",
       "...              ...                 ...       ...       ...   ...\n",
       "663707  118000000012 2018-03-08 16:30:00  0.118551  0.000000     0\n",
       "663708  118000000012 2018-03-08 16:45:00  0.131403  0.000000     0\n",
       "663709  118000000012 2018-03-08 17:00:00  0.083446  0.000000     0\n",
       "663710  118000000012 2018-03-08 17:15:00  0.056625  0.000000     0\n",
       "663711  118000000012 2018-03-08 17:30:00  0.054383  0.000000     0\n",
       "\n",
       "[38370 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enel_data=no_nans\n",
    "\n",
    "# enel_data= pd.read_csv('cleaned_enel_data3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enel_data[\"time\"]= pd.to_datetime(enel_data[\"time\"]) # change to appropriate data type\n",
    "enel_data[\"grid\"]= enel_data[\"solar\"]-enel_data[\"load\"] #create grid value\n",
    "\n",
    "new_enel = [dt.strftime('%Y-%m-%d %H:%M') for dt in \n",
    "       datetime_range(enel_data[\"time\"].min(), enel_data[\"time\"].max(), \n",
    "       timedelta(minutes=15))]\n",
    "\n",
    "new_data=pd.DataFrame(columns=[\"time\"])\n",
    "new_data[\"time\"]=pd.to_datetime(new_enel)\n",
    "\n",
    "house_iter_ids=enel_data[\"dataid\"].unique()\n",
    "\n",
    "\n",
    "final_df=pd.DataFrame(columns=[\"dataid\",\"local_15min\",\"load\",\"solar\",\"grid\"])\n",
    "\n",
    "for house_iter_id in house_iter_ids:\n",
    "\n",
    "    iter_data=enel_data.loc[enel_data[\"dataid\"]==house_iter_id,:]\n",
    "    iter_data=enel_data.merge(new_data, how='outer', left_on='time', right_on='time')\n",
    "    iter_data[\"dataid\"]=house_iter_id\n",
    "    iter_data[\"car1\"]=np.nan\n",
    "    final_df=final_df.append(iter_data, ignore_index=True) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car1</th>\n",
       "      <th>dataid</th>\n",
       "      <th>grid</th>\n",
       "      <th>load</th>\n",
       "      <th>local_15min</th>\n",
       "      <th>solar</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>-0.080569</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>2019-01-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>-0.070977</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>2019-01-07 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>-0.079875</td>\n",
       "      <td>0.079875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019-01-07 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>-0.067573</td>\n",
       "      <td>0.066261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001313</td>\n",
       "      <td>2019-01-07 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>-0.073136</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>2019-01-07 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-07 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-07 03:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-07 03:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-07 03:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-07 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342832 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        car1        dataid      grid      load local_15min     solar  \\\n",
       "0        NaN  118000000008 -0.080569  0.077571         NaN -0.002998   \n",
       "1        NaN  118000000008 -0.070977  0.070403         NaN -0.000574   \n",
       "2        NaN  118000000008 -0.079875  0.079875         NaN  0.000000   \n",
       "3        NaN  118000000008 -0.067573  0.066261         NaN -0.001313   \n",
       "4        NaN  118000000008 -0.073136  0.070168         NaN -0.002968   \n",
       "...      ...           ...       ...       ...         ...       ...   \n",
       "342827   NaN  118000000010       NaN       NaN         NaN       NaN   \n",
       "342828   NaN  118000000010       NaN       NaN         NaN       NaN   \n",
       "342829   NaN  118000000010       NaN       NaN         NaN       NaN   \n",
       "342830   NaN  118000000010       NaN       NaN         NaN       NaN   \n",
       "342831   NaN  118000000010       NaN       NaN         NaN       NaN   \n",
       "\n",
       "                      time  \n",
       "0      2019-01-07 00:00:00  \n",
       "1      2019-01-07 00:15:00  \n",
       "2      2019-01-07 00:30:00  \n",
       "3      2019-01-07 00:45:00  \n",
       "4      2019-01-07 01:00:00  \n",
       "...                    ...  \n",
       "342827 2019-12-07 03:00:00  \n",
       "342828 2019-12-07 03:15:00  \n",
       "342829 2019-12-07 03:30:00  \n",
       "342830 2019-12-07 03:45:00  \n",
       "342831 2019-12-07 04:00:00  \n",
       "\n",
       "[342832 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('cleaned_enel_data4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
