{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "PV1_all = pd.read_csv('Data\\enel_data\\PV_data\\PV1.csv')  #read the PV1 data file to get the solar generation data\n",
    "PV1_filtered = PV1_all[PV1_all['Alarm'] == 0] # Filter out zeros \n",
    "PV1_filtered[\"V\"]=PV1_filtered[\"V\"].apply(pd.to_numeric) #ensure all voltage laues are numeric\n",
    "\n",
    "\n",
    "\n",
    "PV1_filtered = PV1_filtered[PV1_filtered[\"V\"] < 214748360]  #filter out misreadins as enel tells us to in their email\n",
    "PV1_filtered = PV1_filtered[['timestamp','I_shunt','V']]\n",
    "PV1_filtered[\"Power_kW\"]= PV1_filtered['I_shunt']*PV1_filtered['V']/1000 #Power = I*V   (dived by 1000 to get units of kw)\n",
    "PV1_filtered['timestamp'] = pd.to_datetime(PV1_filtered['timestamp'],errors = 'coerce') # change timestamps from strings to datetime objects so we can index and resample. takes like 5 min \n",
    "PV1_filtered.index=PV1_filtered[\"timestamp\"] # set index to be our datetime so we can resample\n",
    "\n",
    "PV1_resampled=PV1_filtered.resample('15T').mean() #resample power reading to the mean of every 15 minute interval\n",
    "\n",
    "#need to isolate the timestamp from the year it was taken so we can merge with the timestamps of a different year for the house load data (since we do not have solar data available for every house)\n",
    "PV1_resampled[\"month1\"]=pd.DatetimeIndex(PV1_resampled.index).month.astype(str) #create month column\n",
    "PV1_resampled[\"day1\"]=pd.DatetimeIndex(PV1_resampled.index).day.astype(str) #create day column\n",
    "PV1_resampled[\"time1\"]=pd.DatetimeIndex(PV1_resampled.index).time.astype(str) # create time column\n",
    "\n",
    "\n",
    "\n",
    "PV1_resampled['marker1'] = PV1_resampled[['month1', 'day1', \"time1\"]].agg('-'.join, axis=1) #create our mark column that we will use to merge this df with the house load df later\n",
    "PV1_resampled=PV1_resampled.drop(columns=[\"I_shunt\",\"V\"]) #we no longer need these values\n",
    "\n",
    "# PV2_all = pd.read_csv('Data\\enel_data\\PV_data\\PV2.csv')\n",
    "# PV2_filtered = PV2_all[PV2_all['Alarm'] == 0] # Filter out zeros \n",
    "# PV2_filtered[\"V\"]=PV2_filtered[\"V\"].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# PV2_filtered = PV2_filtered[PV2_filtered[\"V\"] < 214748360]\n",
    "# PV2_filtered = PV2_filtered[['timestamp','I_shunt','V']]\n",
    "# PV2_filtered[\"Power_kW\"]= PV2_filtered['I_shunt']*PV2_filtered['V']/1000\n",
    "\n",
    "# PV2_filtered['timestamp'] = pd.to_datetime(PV2_filtered['timestamp'],errors = 'coerce') # takes like 5 min \n",
    "# PV2_filtered.index=PV2_filtered[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of files we want to sort through\n",
    "\n",
    "files=[\"file_201907.csv\",\"file_201906.csv\",\"file_201905.csv\",\"file_201904.csv\",\"file_201903.csv\",\"file_201902.csv\",\"file_201901.csv\",\"file_201812.csv\",\"file_201811.csv\",\"file_201810.csv\",\"file_201809.csv\",\"file_201808.csv\"]\n",
    "test=[\"file_201907.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe that will be appended with more house data from the following loops\n",
    "\n",
    "cleaned_data=pd.DataFrame(columns=[\"dataid\",\"local_15min\",\"load\",\"solar\"])\n",
    "\n",
    "# df.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caleb\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012 118000000019]\n",
      "[118000000008 118000000012]\n",
      "[118000000008 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n",
      "[118000000008 118000000010 118000000012]\n"
     ]
    }
   ],
   "source": [
    "for file in files: #iterate through all files given above\n",
    "    \n",
    "    filepath= \"Data\\\\enel_data\\\\NILM_data\\\\\" + file  \n",
    "                      \n",
    "    all_data = pd.read_csv(filepath,delimiter = ';',encoding = 'utf8') #read this file\n",
    "    \n",
    "    serial_buffer=all_data[all_data['Serial_Number'] != \"Serial_Number\"][['Serial_Number','Data','Attiva']] #get rid of erroneous rows\n",
    "    serial_buffer=serial_buffer[serial_buffer[\"Attiva\"].map(type)== int]\n",
    "    \n",
    "    serial_buffer['Serial_Number']=pd.to_numeric(serial_buffer['Serial_Number']) #someserial numbers are given as strings so lets convert all to inttegers\n",
    "    serial_buffer['Data'] = pd.to_datetime(serial_buffer['Data'],errors = 'coerce') # convert timestamp to datetime object.  takes like 5 min \n",
    "    house_ids=serial_buffer[\"Serial_Number\"].unique() #get a list of all Serial Numbers (houses) in this file\n",
    "\n",
    "    print(house_ids)\n",
    "    \n",
    "    for house_id in house_ids: #do a new interation for each separate home that we found in this file\n",
    "        \n",
    "        house= serial_buffer[serial_buffer['Serial_Number'] == house_id] #select data for this home\n",
    "        house = house.rename(columns={'Data':'time','Attiva':'load'}) #load is in mW\n",
    "        house.index = house['time'] #change index \n",
    "        house = house.drop(columns='time')\n",
    "        house[\"load\"] = house[\"load\"].apply(pd.to_numeric, errors='coerce') #make all load values numeric\n",
    "        house = house.resample('15T').mean() #resample house load to average load every 15 min\n",
    "\n",
    "        house['load'] = house['load'] / 1000 / 1000 # convert mW to kW\n",
    "        \n",
    "        \n",
    "        house[\"t_stamp\"]=house.index #make new column of datetime we can break apart\n",
    "        \n",
    "        #same as PV data: make a marker we will use to merge the two df by taking the year out of the timestamp\n",
    "        house[\"month2\"]=pd.DatetimeIndex(house.index).month.astype(str)\n",
    "        house[\"day2\"]=pd.DatetimeIndex(house.index).day.astype(str)\n",
    "        house[\"timey2\"]=pd.DatetimeIndex(house.index).time.astype(str)\n",
    "\n",
    "        house['marker2'] = house[['month2', 'day2', \"timey2\"]].agg('-'.join, axis=1) #create the second marker\n",
    "        \n",
    "        merged= pd.merge(house,PV1_resampled, how='inner', left_on=\"marker2\", right_on=\"marker1\") #merge the two df using the markers we made. data from the same realtive date and time interval will be combined\n",
    "        df_append= merged.loc[:,[\"Serial_Number\",\"t_stamp\",\"load\",\"Power_kW\"]] #select the columns we want to keep\n",
    "        df_append = df_append.rename(columns={'Serial_Number':'dataid','t_stamp':'local_15min','Power_kW':'solar'})\n",
    "        \n",
    "        cleaned_data=cleaned_data.append(df_append, ignore_index=True) #add the merged load and solar data for all homes in this file to the larger df we are creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans=cleaned_data[pd.notna(cleaned_data[\"dataid\"])]\n",
    "no_nans = no_nans.rename(columns={'local_15min':'time'})\n",
    "no_nans[\"car1\"] = 0\n",
    "no_nans[\"dataid\"]= no_nans[\"dataid\"].apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataid</th>\n",
       "      <th>time</th>\n",
       "      <th>load</th>\n",
       "      <th>solar</th>\n",
       "      <th>car1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:00:00</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:15:00</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:30:00</td>\n",
       "      <td>0.079875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 00:45:00</td>\n",
       "      <td>0.066261</td>\n",
       "      <td>-0.001313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>118000000008</td>\n",
       "      <td>2019-01-07 01:00:00</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663707</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 16:30:00</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663708</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 16:45:00</td>\n",
       "      <td>0.131403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663709</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:00:00</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663710</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:15:00</td>\n",
       "      <td>0.056625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663711</td>\n",
       "      <td>118000000012</td>\n",
       "      <td>2018-03-08 17:30:00</td>\n",
       "      <td>0.054383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38370 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataid                time      load     solar  car1\n",
       "0       118000000008 2019-01-07 00:00:00  0.077571 -0.002998     0\n",
       "1       118000000008 2019-01-07 00:15:00  0.070403 -0.000574     0\n",
       "2       118000000008 2019-01-07 00:30:00  0.079875  0.000000     0\n",
       "3       118000000008 2019-01-07 00:45:00  0.066261 -0.001313     0\n",
       "4       118000000008 2019-01-07 01:00:00  0.070168 -0.002968     0\n",
       "...              ...                 ...       ...       ...   ...\n",
       "663707  118000000012 2018-03-08 16:30:00  0.118551  0.000000     0\n",
       "663708  118000000012 2018-03-08 16:45:00  0.131403  0.000000     0\n",
       "663709  118000000012 2018-03-08 17:00:00  0.083446  0.000000     0\n",
       "663710  118000000012 2018-03-08 17:15:00  0.056625  0.000000     0\n",
       "663711  118000000012 2018-03-08 17:30:00  0.054383  0.000000     0\n",
       "\n",
       "[38370 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot=no_nans[no_nans[\"dataid\"]==118000000008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7a29c396ff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mto_plot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"load\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'to_plot' is not defined"
     ]
    }
   ],
   "source": [
    "to_plot.plot(\"time\",[\"load\"]).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans.to_csv('cleaned_enel_data3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
